# -*- coding: utf-8 -*-
"""Decision tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vUm8pWvyrueKfcOnVXxHnOPp-sVIi6e6
"""

#Importing librabies

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder # to process the data (encoding)

from sklearn.model_selection import train_test_split, GridSearchCV  # this will allow us to split the data into training and testing, and GridSearchCV is for hypermeter tuning

from sklearn.tree import DecisionTreeClassifier # this is the decision tree classifier itself, we will use this to build the model

from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, precision_score, recall_score, classification_report # all these are to check the model, get the score and stuff

# We can use data that is categorical or numerical, decision tree can handle both. This model also allows us to understand the resaon behind the decision it is making.
data = pd.read_csv('/content/drive/MyDrive/bank.csv')
data.head()

# It is recomended to the EDA first in order to understand the dataset and if you haven't done that, read the next comment.
# we need to check if there are any missing values and the number of columns having objects and numerical columns in order to preprocess the data.
data.isnull().sum()

# Checking the columns with data type object.
for cols in data.columns:
  if data[cols].dtype == 'object':
    print(cols, ':', data[cols].nunique())

# Preparing the data for ML model and we are using Label encoder to do that, it converts categorical data into numerical data.
# We are not going to scale the data as decision tree is not affected by the scale of the columns or features.

encoder = LabelEncoder()
for cols in data.columns:
  if data[cols].dtype == 'O':
    data[cols] = encoder.fit_transform(data[cols])

data.head()

# Separating dependent and independent column. The aim is to put the answer/result/predict coulmn separately.

x = data.drop(['deposit'], axis=1)
y = data['deposit']

# Now, this is training and testing split, decision tree model will be trained in some part of the data and to check if it is working or not, we will use testing data to test the model.

xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.3, random_state=42)

# Now we can create the ML model. Here the criterion can be 'gini', 'log loss' and 'entropy'. By default it is gini and we will not be changing it for this model.
# Rest of the details are available in the documentation.
''' DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,
                           min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None) '''

model = DecisionTreeClassifier(max_depth=5, random_state=42)
model.fit(xtrain, ytrain)
train_score = model.score(xtrain, ytrain)

#Now let's use the model to make the prediction.
ypred = model.predict(xtest)

#training score
print('Training score is' , round(train_score*100,2), '%')

# testing score
print('Testing Score:', round(accuracy_score(ytest, ypred)*100, 2), '%')
print('Recall Score:', round(recall_score(ytest, ypred)*100, 2), '%')
print('Precision Score:', round(precision_score(ytest, ypred)*100, 2), '%')

# We can either calculate the training, testing, recall score like we did in the previous cell or we can just create a confusion matrix adn get all the score data at once.

res = ConfusionMatrixDisplay.from_predictions(ytest, ypred, display_labels=['No', 'Yes'])
print('Classification Report\n\n', classification_report(ytest, ypred))

# This model that we have created is not a good model as it is not able to make correct prediction or we can say the scores are not better.
# In order to make it more accurate, we will have to change the parameters and re run it again and again or we could just use an inbuilt feature called as hypermeter tuning and
# let the system calculate the best parameters.

h_para = {'criterion': ['gini', 'entropy', 'log_loss'],
          'max_depth': [6,7,8,9,10,11,12],
          'min_samples_split': [2,4,6,8],
          'min_samples_leaf': [1,2,3,4]}

model1 = DecisionTreeClassifier(random_state= 42)
x = GridSearchCV(estimator = model1,
              param_grid = h_para,
              n_jobs = -1,
              )
x.fit(xtrain, ytrain)

best_para = x.best_params_
print("The best parameter for the model is:\n", best_para)

# Now we can use the best parameters to tune our model
model_final = DecisionTreeClassifier(**best_para, random_state=42)

# fit the data onto the final model
model_final.fit(xtrain, ytrain)

ypred_final = model_final.predict(xtest)

res_final = ConfusionMatrixDisplay.from_predictions(ytest, ypred_final, display_labels=['No', 'Yes'])
print('Final classification Report\n\n', classification_report(ytest, ypred))

